# lib/datasets/cleftlip.py
# ------------------------------------------------------------------
# 6-landmark cleft-lip dataset for HRNet â€“ drop-in replacement
# ------------------------------------------------------------------
import os, warnings, numpy as np, pandas as pd
import torch
from torch.utils.data import Dataset
import torchvision.transforms as T
from PIL import Image
import matplotlib.pyplot as plt
from tqdm import tqdm

# HRNet åŸç”Ÿç”Ÿæˆçƒ­å›¾å‡½æ•°
from ..utils.transforms import generate_target

class CleftLip(Dataset):
    """
    ROOT/
        å¤„ç†åå›¾ç‰‡/
        å”‡è£‚æ ‡æ³¨åˆ†æç»“æœ_512px.csv
        æ•°æ®é›†åˆ’åˆ†.csv
    """
    NUM_JOINTS = 6
    SIGMA      = 2.0                       # è®¾ä¸ºåˆç†çš„é€‚ä¸­å€¼
    KP_NAMES   = ['E1','E2','I1','I2','N_left','N_right']

    COL = {                                 # ä¸­æ–‡åˆ—åæ˜ å°„
        'SPLIT_ID'   : 'å›¾åƒID',
        'SPLIT_TYPE' : 'æ•°æ®é›†ç±»å‹',
        'IMG_ID'     : 'å›¾åƒID',
        'FILENAME'   : 'åŸå§‹æ–‡ä»¶å',
        'KPT_NAME'   : 'å…³é”®ç‚¹',
        'X'          : 'å½’ä¸€åŒ–X',
        'Y'          : 'å½’ä¸€åŒ–Y',
    }

    # --------------------------------------------------------------
    def __init__(self, cfg, is_train=True, transform=None):
        split_tag   = 'è®­ç»ƒé›†' if is_train else 'éªŒè¯é›†'
        root        = cfg.DATASET.ROOT.rstrip('/\\')

        anno_csv    = os.path.join(root, cfg.DATASET.TRAINSET)
        split_csv   = os.path.join(root, cfg.DATASET.TESTSET)
        self.img_dir= os.path.join(root, cfg.DATASET.IMAGE_DIR)

        # ä»é…ç½®è¯»å–å›¾åƒå°ºå¯¸ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç 
        self.IMAGE_SIZE = tuple(int(x) for x in cfg.MODEL.IMAGE_SIZE)

        # è¯»å– CSV å¹¶å»æ‰ç©ºæ ¼
        strip = lambda df: df.applymap(lambda x: x.strip() if isinstance(x,str) else x)
        self.anno_df  = strip(pd.read_csv(anno_csv, dtype=str))
        self.split_df = strip(pd.read_csv(split_csv, dtype=str))

        # ------------------------------------------------------------------
        # å»º ID â†’ æ–‡ä»¶å æ˜ å°„
        id2file = {}
        if self.COL['FILENAME'] in self.split_df.columns:
            id2file.update(
                self.split_df.dropna(subset=[self.COL['FILENAME']])
                .set_index(self.COL['SPLIT_ID'])[self.COL['FILENAME']].to_dict()
            )
        id2file.update(                           # ç”¨ anno_df è¡¥å…¨ç¼ºå¤±
            self.anno_df.dropna(subset=[self.COL['FILENAME']])
            .drop_duplicates(subset=[self.COL['IMG_ID']])
            .set_index(self.COL['IMG_ID'])[self.COL['FILENAME']].to_dict()
        )
        # ------------------------------------------------------------------
        # å½“å‰ split çš„æ‰€æœ‰æ ·æœ¬
        split_ids = (
            self.split_df[self.split_df[self.COL['SPLIT_TYPE']] == split_tag]
            [self.COL['SPLIT_ID']].tolist()
        )

        # è¯»å– cfg.MODEL.HEATMAP_SIZEï¼ˆä¿è¯ä¸ç½‘ç»œä¸€è‡´ï¼‰
        self.HEATMAP_SIZE = tuple(int(x) for x in cfg.MODEL.HEATMAP_SIZE)

        # æ„å»ºæ ·æœ¬åˆ—è¡¨
        self.samples = []
        for img_id in tqdm(split_ids, desc=f"åŠ è½½{split_tag}æ•°æ®"):
            fname = id2file.get(img_id, '')
            if not fname:               # æ²¡æœ‰æ–‡ä»¶å
                continue

            # ------- å…³é”®ç‚¹ -------
            grp = self.anno_df[self.anno_df[self.COL['FILENAME']] == fname]
            if grp.empty:
                continue
            kp = np.full((self.NUM_JOINTS, 2), -1, np.float32)
            for i, name in enumerate(self.KP_NAMES):
                row = grp[grp[self.COL['KPT_NAME']] == name]
                if len(row):
                    kp[i] = [float(row[self.COL['X']].iloc[0]),
                             float(row[self.COL['Y']].iloc[0])]

            # ------- å®šä½å›¾ç‰‡ -------
            img_path = os.path.join(self.img_dir, fname)
            if not os.path.isfile(img_path):
                tail = fname.split('-', 1)[-1]
                hits = [f for f in os.listdir(self.img_dir)
                        if f.lower().endswith(tail.lower())]
                if hits:
                    img_path = os.path.join(self.img_dir, hits[0])

            if os.path.isfile(img_path):
                self.samples.append((img_path, kp))
            else:
                warnings.warn(f'âš ï¸ æ‰¾ä¸åˆ°å›¾ç‰‡: {img_path}  â€” å·²è·³è¿‡')

        if not self.samples:
            raise RuntimeError('ğŸ’¥ æ•°æ®é›†åˆå§‹åŒ–åä»ä¸ºç©ºï¼Œè¯·æ£€æŸ¥è·¯å¾„/æ–‡ä»¶åï¼')

        # å›¾åƒ transform
        self.transform = transform or T.Compose([
            T.Resize(self.IMAGE_SIZE),
            T.ToTensor(),
            T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])
        ])

    # --------------------------------------------------------------
    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, kp_norm = self.samples[idx]

        # -------- å›¾åƒ --------
        img = Image.open(img_path).convert('RGB')
        img = self.transform(img)

        # -------- çƒ­å›¾ --------
        target = np.zeros((self.NUM_JOINTS, *self.HEATMAP_SIZE), np.float32)
        for j in range(self.NUM_JOINTS):
            x_norm, y_norm = kp_norm[j]
            if x_norm < 0 or y_norm < 0:
                continue                      # ç¼ºå¤±ç‚¹
            # æ˜ç¡®æŒ‡å®šxå’Œyå¯¹åº”å®½åº¦å’Œé«˜åº¦ï¼Œé¿å…HEATMAP_SIZEæ ¼å¼å˜åŒ–å¯¼è‡´çš„åæ ‡æ··æ·†
            # ä¿®æ­£ï¼šç¡®ä¿xä½¿ç”¨width(W)ï¼Œyä½¿ç”¨height(H)
            heatmap_width, heatmap_height = self.HEATMAP_SIZE[1], self.HEATMAP_SIZE[0]  # W, H
            pt = np.array([x_norm * heatmap_width,  # x â†” width (W)
                           y_norm * heatmap_height]) # y â†” height (H)
            target[j] = generate_target(target[j], pt, self.SIGMA)

        target = torch.from_numpy(target)

        # å°†å½’ä¸€åŒ–åæ ‡è½¬æ¢ä¸ºåƒç´ åæ ‡ï¼Œä¸predsåæ ‡ç³»ç»ŸåŒ¹é…
        img_width, img_height = self.IMAGE_SIZE[1], self.IMAGE_SIZE[0]  # W, H
        pix_pts = kp_norm * np.array([img_width, img_height])   # (6,2) å½’ä¸€åŒ– â†’ åƒç´ åæ ‡
        
        # ä¿®æ­£metaé‡Œçš„centerå’Œscaleï¼Œç¡®ä¿æ­£ç¡®è¡¨ç¤ºåŸå›¾åˆ°çƒ­å›¾çš„è½¬æ¢å…³ç³»
        # meta é‡Œä¿å­˜åƒç´ åæ ‡ï¼Œä¸predsåŒ¹é…ï¼Œç»™ NME è®¡ç®—ç”¨
        meta = {
            'index' : idx,
            'pts'   : torch.from_numpy(pix_pts).float()      # ä½¿ç”¨åƒç´ åæ ‡
        }
        return img, target, meta

def visualize_keypoints(image, keypoints, predictions=None, save_path=None, is_normalized=True, image_size=(512, 512)):
    """å¯è§†åŒ–å…³é”®ç‚¹é¢„æµ‹ç»“æœ
    
    å‚æ•°:
        image: å›¾åƒå¼ é‡æˆ–PILå›¾åƒ
        keypoints: å…³é”®ç‚¹åæ ‡ [num_keypoints, 2] 
        predictions: é¢„æµ‹å…³é”®ç‚¹åæ ‡ [num_keypoints, 2]
        save_path: ä¿å­˜è·¯å¾„ï¼Œè‹¥ä¸ºNoneåˆ™æ˜¾ç¤ºå›¾åƒ
        is_normalized: è¾“å…¥åæ ‡æ˜¯å¦ä¸ºå½’ä¸€åŒ–åæ ‡(0-1)ï¼Œè‹¥ä¸ºFalseåˆ™ä¸ºåƒç´ åæ ‡
        image_size: å›¾åƒå°ºå¯¸ï¼Œç”¨äºå½’ä¸€åŒ–/åå½’ä¸€åŒ–åæ ‡ï¼Œé»˜è®¤ä¸º(512, 512)
    """
    try:
        # å¦‚æœæ˜¯å¼ é‡ï¼Œè½¬æ¢ä¸ºnumpyå¹¶è¿›è¡Œæ­£ç¡®çš„åå½’ä¸€åŒ–
        if isinstance(image, torch.Tensor):
            # é¦–å…ˆç§»åŠ¨åˆ°CPUå¹¶è½¬æ¢ä¸ºnumpyæ•°ç»„
            image = image.cpu().numpy().transpose(1, 2, 0)
            
            # åå½’ä¸€åŒ– - æ¢å¤åŸå§‹å›¾åƒæ˜¾ç¤ºæ•ˆæœ
            mean = np.array([0.485, 0.456, 0.406])
            std = np.array([0.229, 0.224, 0.225])
            image = std * image + mean
            
            # è£å‰ªå€¼åˆ°0-1èŒƒå›´
            image = np.clip(image, 0, 1)
            
            # è½¬æ¢ä¸º0-255çš„RGBå›¾åƒ
            image = (image * 255).astype(np.uint8)
        
        # åˆ›å»ºmatplotlibå›¾å½¢
        plt.figure(figsize=(10, 10))
        plt.imshow(image)
        
        # å®šä¹‰å…³é”®ç‚¹åç§°å’Œé¢œè‰²
        keypoint_names = ['E1', 'E2', 'I1', 'I2', 'N_left', 'N_right']
        colors = ['r', 'r', 'g', 'g', 'b', 'b']
        
        # è½¬æ¢å…³é”®ç‚¹åæ ‡è‡³åƒç´ åæ ‡
        h, w = image.shape[:2]
        img_w, img_h = image_size
        
        # åæ ‡ç³»è½¬æ¢ï¼šç¡®ä¿å¤„ç†åƒç´ åæ ‡
        if keypoints is not None:
            keypoints = keypoints.cpu().numpy() if isinstance(keypoints, torch.Tensor) else keypoints
            # å¦‚æœæ˜¯å½’ä¸€åŒ–åæ ‡ï¼Œè½¬æ¢ä¸ºåƒç´ åæ ‡
            if is_normalized:
                keypoints = keypoints.copy() * np.array([img_w, img_h])
        
        if predictions is not None:
            predictions = predictions.cpu().numpy() if isinstance(predictions, torch.Tensor) else predictions
            # å¦‚æœpredictionsæ˜¯åƒç´ åæ ‡è€Œéœ€è¦å½’ä¸€åŒ–åæ ‡ï¼Œæˆ–åä¹‹ï¼Œè¿›è¡Œè½¬æ¢
            if is_normalized:
                predictions = predictions.copy() * np.array([img_w, img_h])
        
        # ç»˜åˆ¶çœŸå®å…³é”®ç‚¹ - ä½¿ç”¨å®å¿ƒå¤§åœ†ç‚¹
        if keypoints is not None:
            for i, (kp, name, color) in enumerate(zip(keypoints, keypoint_names, colors)):
                x, y = int(kp[0] * w / img_w), int(kp[1] * h / img_h)  # è°ƒæ•´åˆ°å®é™…å›¾åƒå¤§å°
                plt.scatter(x, y, c=color, s=40, marker='o', alpha=0.7)
                plt.text(x+5, y+5, name, color=color, fontsize=12, weight='bold')
        
        # ç»˜åˆ¶é¢„æµ‹å…³é”®ç‚¹ - ä½¿ç”¨Xå½¢ï¼Œæ›´å®¹æ˜“ä¸çœŸå®å€¼åŒºåˆ†
        if predictions is not None:
            for i, (kp, name) in enumerate(zip(predictions, keypoint_names)):
                x, y = int(kp[0] * w / img_w), int(kp[1] * h / img_h)  # è°ƒæ•´åˆ°å®é™…å›¾åƒå¤§å°
                plt.scatter(x, y, c='yellow', s=30, marker='x', linewidths=2)
                plt.text(x-15, y-15, f'pred_{name}', color='yellow', fontsize=10, weight='bold')
                
                # æ·»åŠ è¿çº¿æ˜¾ç¤ºé¢„æµ‹ç‚¹ä¸çœŸå®ç‚¹ä¹‹é—´çš„åå·®
                if keypoints is not None:
                    true_x, true_y = int(keypoints[i][0] * w / img_w), int(keypoints[i][1] * h / img_h)
                    plt.plot([x, true_x], [y, true_y], 'y--', alpha=0.7, linewidth=1)
                    
                    # è®¡ç®—å¹¶æ˜¾ç¤ºæ¬§æ°è·ç¦»
                    dist = np.sqrt((x-true_x)**2 + (y-true_y)**2)
                    mid_x, mid_y = (x + true_x) // 2, (y + true_y) // 2
                    plt.text(mid_x, mid_y, f'{dist:.1f}px', color='white', fontsize=8, 
                             bbox=dict(facecolor='black', alpha=0.5))
        
        # æ·»åŠ å›¾ä¾‹
        plt.scatter([], [], c='r', s=40, marker='o', alpha=0.7, label='çœŸå®çœ¼éƒ¨å…³é”®ç‚¹(E1,E2)')
        plt.scatter([], [], c='g', s=40, marker='o', alpha=0.7, label='çœŸå®å†…å˜´è§’å…³é”®ç‚¹(I1,I2)')
        plt.scatter([], [], c='b', s=40, marker='o', alpha=0.7, label='çœŸå®é¼»ç¿¼å…³é”®ç‚¹(N_left,N_right)')
        plt.scatter([], [], c='yellow', s=30, marker='x', label='é¢„æµ‹å…³é”®ç‚¹')
        plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
        
        # æ·»åŠ æ ‡é¢˜æ˜¾ç¤ºå‡†ç¡®ç‡ä¿¡æ¯
        if predictions is not None and keypoints is not None:
            # è®¡ç®—åƒç´ åæ ‡ä¸‹çš„æ¬§æ°è·ç¦»
            distances = []
            for i in range(len(keypoints)):
                true_x, true_y = int(keypoints[i][0] * w / img_w), int(keypoints[i][1] * h / img_h)
                pred_x, pred_y = int(predictions[i][0] * w / img_w), int(predictions[i][1] * h / img_h)
                dist = np.sqrt((pred_x-true_x)**2 + (pred_y-true_y)**2)
                distances.append(dist)
            
            avg_dist = np.mean(distances)
            plt.title(f'å¹³å‡å…³é”®ç‚¹è¯¯å·®: {avg_dist:.1f}åƒç´ ', fontsize=14)
        
        # ä¿å­˜æˆ–æ˜¾ç¤ºå›¾åƒ
        if save_path:
            plt.savefig(save_path, bbox_inches='tight')
            plt.close()
        else:
            plt.show()
    except Exception as e:
        print(f"å¯è§†åŒ–è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        # å‡ºé”™æ—¶ä¸ä¸­æ–­è®­ç»ƒæµç¨‹

def calculate_accuracy(predictions, targets, threshold=0.05, is_normalized=True, image_size=(512, 512)):
    """è®¡ç®—å…³é”®ç‚¹æ£€æµ‹å‡†ç¡®ç‡
    
    å‚æ•°:
        predictions: é¢„æµ‹å…³é”®ç‚¹åæ ‡ [batch_size, num_keypoints, 2] 
        targets: çœŸå®å…³é”®ç‚¹åæ ‡ [batch_size, num_keypoints, 2]
        threshold: æ­£ç¡®é¢„æµ‹çš„é˜ˆå€¼ï¼Œé»˜è®¤ä¸º0.05ï¼ˆå½’ä¸€åŒ–ç©ºé—´ä¸­è·ç¦»ï¼‰
        is_normalized: è¾“å…¥åæ ‡æ˜¯å¦ä¸ºå½’ä¸€åŒ–åæ ‡(0-1)ï¼Œè‹¥ä¸ºFalseåˆ™ä¸ºåƒç´ åæ ‡
        image_size: å›¾åƒå°ºå¯¸ï¼Œç”¨äºå½’ä¸€åŒ–åæ ‡ï¼Œé»˜è®¤ä¸º(512, 512)
    
    è¿”å›:
        å‡†ç¡®ç‡ç™¾åˆ†æ¯”
    """
    # ç¡®ä¿è¾“å…¥æ˜¯numpyæ•°ç»„
    if isinstance(predictions, torch.Tensor):
        predictions = predictions.cpu().detach().numpy()
    if isinstance(targets, torch.Tensor):
        targets = targets.cpu().detach().numpy()
    
    # å¦‚æœæ˜¯åƒç´ åæ ‡ï¼Œéœ€è¦å½’ä¸€åŒ–
    if not is_normalized:
        img_w, img_h = image_size
        predictions = predictions.copy() / np.array([img_w, img_h])
        targets = targets.copy() / np.array([img_w, img_h])
    
    # è®¡ç®—å½’ä¸€åŒ–ç©ºé—´çš„æ¬§æ°è·ç¦»
    distances = np.sqrt(np.sum((predictions - targets) ** 2, axis=2))
    
    # å‡†ç¡®ç‡ - å®šä¹‰ä¸ºå½’ä¸€åŒ–è·ç¦»å°äºé˜ˆå€¼çš„å…³é”®ç‚¹ç™¾åˆ†æ¯”
    accuracy = np.mean(distances < threshold) * 100
    
    return accuracy
